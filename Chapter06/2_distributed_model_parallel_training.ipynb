{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Model Parallel Training on Amazon SageMaker\n",
    "In this notebook we will use a Visual transformer to do image classification `horse or human` data from https://laurencemoroney.com/datasets.html. We will download both training and validation dataset provided on the site. \n",
    "\n",
    "Note: \n",
    "- Kernel: `PyTorch 1.8 Python 3.6 CPU Optimized)`\n",
    "- Instance Type: `ml.m5.xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (2.84.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.85.0.tar.gz (521 kB)\n",
      "\u001b[K     |████████████████████████████████| 521 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.21.36)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.19.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (3.15.8)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (4.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (20.9)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.2.7)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.36 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (1.24.36)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore<1.25.0,>=1.24.36->boto3>=1.20.21->sagemaker) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.25.0,>=1.24.36->boto3>=1.20.21->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.11 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.11.1)\n",
      "Requirement already satisfied: pox>=0.2.9 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.2.9)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.3)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.85.0-py2.py3-none-any.whl size=720822 sha256=aa5805fed2aef03bd3b9e4a9d4c194c8a113121a115a92a8498660406ac29e09\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o_dehoet/wheels/4d/28/db/508f5f0c9a229fcdaf43d047fc16d21c51fd3e7e6d674d464e\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.84.0\n",
      "    Uninstalling sagemaker-2.84.0:\n",
      "      Successfully uninstalled sagemaker-2.84.0\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Successfully installed sagemaker-2.85.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# update sagemaker\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.85.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure version is greater or equal to '2.84.0'\n",
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  142M  100  142M    0     0  96.2M      0  0:00:01  0:00:01 --:--:-- 96.2M\n"
     ]
    }
   ],
   "source": [
    "## Download data\n",
    "!curl -o train.zip https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.9M  100 10.9M    0     0  20.3M      0 --:--:-- --:--:-- --:--:-- 20.3M\n"
     ]
    }
   ],
   "source": [
    "!curl -o validation.zip https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unzip file\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"train.zip\",\"r\") as train_zip_ref:\n",
    "    train_zip_ref.extractall(\"data/train\")\n",
    "    \n",
    "with zipfile.ZipFile(\"validation.zip\",\"r\") as val_zip_ref:\n",
    "    val_zip_ref.extractall(\"data/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to High Resolution\n",
    "We will start with converting our images to High Resolution using HuggingFace model [EdsrModel](#https://huggingface.co/eugenesiow/edsr-base) from `super-image` library. \n",
    "Please note that this step is optional, and reason for doing this is to mimick the real world image datasets for High Performance Computing, where image size might be in mega bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "     |████████████████████████████████| 325 kB 8.0 MB/s            \n",
      "\u001b[?25hCollecting super-image\n",
      "  Downloading super_image-0.1.6-py3-none-any.whl (85 kB)\n",
      "     |████████████████████████████████| 85 kB 109.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (2.26.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "     |████████████████████████████████| 211 kB 110.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets) (1.19.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets) (21.3)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 109.6 MB/s            \n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 104.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets) (4.8.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (2021.7.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
      "     |████████████████████████████████| 831.4 MB 159.8 MB/s            \n",
      "\u001b[?25hCollecting super-image\n",
      "  Downloading super_image-0.1.5-py3-none-any.whl (84 kB)\n",
      "     |████████████████████████████████| 84 kB 102.8 MB/s            \n",
      "\u001b[?25h  Downloading super_image-0.1.4-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 117.0 MB/s            \n",
      "\u001b[?25h  Downloading super_image-0.1.3-py3-none-any.whl (43 kB)\n",
      "     |████████████████████████████████| 43 kB 78.7 MB/s             \n",
      "\u001b[?25h  Downloading super_image-0.1.2-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 100.1 MB/s           \n",
      "\u001b[?25h  Downloading super_image-0.1.1-py3-none-any.whl (32 kB)\n",
      "  Downloading super_image-0.1.0-py3-none-any.whl (27 kB)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
      "     |████████████████████████████████| 312 kB 106.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
      "     |████████████████████████████████| 311 kB 141.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
      "     |████████████████████████████████| 312 kB 59.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
      "     |████████████████████████████████| 311 kB 109.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
      "     |████████████████████████████████| 311 kB 141.3 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 132.3 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
      "     |████████████████████████████████| 298 kB 112.2 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
      "     |████████████████████████████████| 298 kB 56.3 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 119.7 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 90.6 MB/s            \n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.19\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "     |████████████████████████████████| 56 kB 107.9 MB/s            \n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 137.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 124.5 MB/s            \n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 136.8 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 93.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
      "     |████████████████████████████████| 285 kB 137.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "     |████████████████████████████████| 270 kB 134.0 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
      "     |████████████████████████████████| 269 kB 128.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "     |████████████████████████████████| 264 kB 141.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /opt/conda/lib/python3.6/site-packages (from datasets) (4.61.2)\n",
      "Collecting torchvision==0.10.0\n",
      "  Downloading torchvision-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (22.1 MB)\n",
      "     |████████████████████████████████| 22.1 MB 34.6 MB/s            \n",
      "\u001b[?25hCollecting h5py==3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 63.5 MB/s            \n",
      "\u001b[?25hCollecting opencv-python==4.5.2.54\n",
      "  Downloading opencv_python-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (51.0 MB)\n",
      "     |████████████████████████████████| 51.0 MB 943 kB/s            \n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.13-py3-none-any.whl (38 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch==1.9.0->super-image) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.10.0->super-image) (8.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: torch, cached-property, xxhash, torchvision, opencv-python, huggingface-hub, h5py, super-image, datasets\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.1\n",
      "    Uninstalling torch-1.8.1:\n",
      "      Successfully uninstalled torch-1.8.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.1\n",
      "    Uninstalling torchvision-0.9.1:\n",
      "      Successfully uninstalled torchvision-0.9.1\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.4.60\n",
      "    Uninstalling opencv-python-4.5.4.60:\n",
      "      Successfully uninstalled opencv-python-4.5.4.60\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.8.0\n",
      "    Uninstalling h5py-2.8.0:\n",
      "      Successfully uninstalled h5py-2.8.0\n",
      "Successfully installed cached-property-1.5.2 datasets-1.11.0 h5py-3.1.0 huggingface-hub-0.0.13 opencv-python-4.5.2.54 super-image-0.1.6 torch-1.9.0 torchvision-0.10.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (2.72.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.82.2.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 9.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "     |████████████████████████████████| 49 kB 94.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.20.24)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.19.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (4.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (1.23.24)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.24->boto3>=1.20.21->sagemaker) (1.26.6)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.82.2-py2.py3-none-any.whl size=719305 sha256=2fc639a13c6d919237b9c7853d92a18c15f16bd2363e63691c7e9f8d2be2f5e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0u7vzt5x/wheels/a2/6f/5e/f1463fedfe4a5ef991b54e63d3fa7824970260aeb605872e2d\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: attrs, sagemaker\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.72.0\n",
      "    Uninstalling sagemaker-2.72.0:\n",
      "      Successfully uninstalled sagemaker-2.72.0\n",
      "Successfully installed attrs-20.3.0 sagemaker-2.82.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets super-image\n",
    "!python3 -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using `horse-or-human` dataset from here, which has image size of about `178KB`, we will convert these images to Higher resolution and `resulting size would be close to 2MB`. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@InProceedings{Lim_2017_CVPR_Workshops,\n",
    "  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n",
    "  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n",
    "  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n",
    "  month = {July},\n",
    "  year = {2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/eugenesiow/edsr-base/resolve/main/pytorch_model_4x.pt\n"
     ]
    }
   ],
   "source": [
    "from super_image import EdsrModel, ImageLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "folder_dir = \"data/validation/\"\n",
    "for folder in os.listdir(folder_dir):\n",
    "    folder_path = f'{folder_dir}{folder}'\n",
    "    for image_file in os.listdir(folder_path):\n",
    "        path = f'{folder_path}/{image_file}'\n",
    "        image = Image.open(path)\n",
    "        inputs = ImageLoader.load_image(image)\n",
    "        preds = model(inputs)\n",
    "        ImageLoader.save_image(preds, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size is : 0.481116 MB\n"
     ]
    }
   ],
   "source": [
    "# quick check on image size for the last image converted by the model.\n",
    "import os\n",
    "\n",
    "file_size = os.path.getsize(path)\n",
    "print(\"File Size is :\", file_size/1000000, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Duplicate files to increase number of images for testing purpose in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied ./data/validation/horses/1_horse5-235.png\n",
      "copied ./data/validation/horses/2_horse5-550.png\n",
      "copied ./data/validation/horses/3_horse1-554.png\n",
      "copied ./data/validation/horses/4_horse6-153.png\n",
      "copied ./data/validation/horses/5_horse5-514.png\n",
      "copied ./data/validation/horses/6_horse3-255.png\n",
      "copied ./data/validation/horses/7_horse1-455.png\n",
      "copied ./data/validation/horses/8_horse5-181.png\n",
      "copied ./data/validation/horses/9_horse2-544.png\n",
      "copied ./data/validation/horses/10_horse5-002.png\n",
      "copied ./data/validation/horses/11_horse1-204.png\n",
      "copied ./data/validation/horses/12_horse1-105.png\n",
      "copied ./data/validation/horses/13_horse4-102.png\n",
      "copied ./data/validation/horses/14_horse1-411.png\n",
      "copied ./data/validation/horses/15_horse6-089.png\n",
      "copied ./data/validation/horses/16_horse1-510.png\n",
      "copied ./data/validation/horses/17_horse5-275.png\n",
      "copied ./data/validation/horses/18_horse4-159.png\n",
      "copied ./data/validation/horses/19_horse3-484.png\n",
      "copied ./data/validation/horses/20_horse2-368.png\n",
      "copied ./data/validation/horses/21_horse6-544.png\n",
      "copied ./data/validation/horses/22_horse2-269.png\n",
      "copied ./data/validation/horses/23_horse5-402.png\n",
      "copied ./data/validation/horses/24_horse4-043.png\n",
      "copied ./data/validation/horses/25_horse2-441.png\n",
      "copied ./data/validation/horses/26_horse4-599.png\n",
      "copied ./data/validation/horses/27_horse3-521.png\n",
      "copied ./data/validation/horses/28_horse5-123.png\n",
      "copied ./data/validation/horses/29_horse4-403.png\n",
      "copied ./data/validation/horses/30_horse3-440.png\n",
      "copied ./data/validation/horses/31_horse1-127.png\n",
      "copied ./data/validation/horses/32_horse3-026.png\n",
      "copied ./data/validation/horses/33_horse5-303.png\n",
      "copied ./data/validation/horses/34_horse5-060.png\n",
      "copied ./data/validation/horses/35_horse5-589.png\n",
      "copied ./data/validation/horses/36_horse4-232.png\n",
      "copied ./data/validation/horses/37_horse1-000.png\n",
      "copied ./data/validation/horses/38_horse5-192.png\n",
      "copied ./data/validation/horses/39_horse2-294.png\n",
      "copied ./data/validation/horses/40_horse6-218.png\n",
      "copied ./data/validation/horses/41_horse1-484.png\n",
      "copied ./data/validation/horses/42_horse2-069.png\n",
      "copied ./data/validation/horses/43_horse2-582.png\n",
      "copied ./data/validation/horses/44_horse5-400.png\n",
      "copied ./data/validation/horses/45_horse4-302.png\n",
      "copied ./data/validation/horses/46_horse5-103.png\n",
      "copied ./data/validation/horses/47_horse1-224.png\n",
      "copied ./data/validation/horses/48_horse3-141.png\n",
      "copied ./data/validation/horses/49_horse5-488.png\n",
      "copied ./data/validation/horses/50_horse1-170.png\n",
      "copied ./data/validation/horses/51_horse2-412.png\n",
      "copied ./data/validation/horses/52_horse4-014.png\n",
      "copied ./data/validation/horses/53_horse4-588.png\n",
      "copied ./data/validation/horses/54_horse2-201.png\n",
      "copied ./data/validation/horses/55_horse3-198.png\n",
      "copied ./data/validation/horses/56_horse3-099.png\n",
      "copied ./data/validation/horses/57_horse5-259.png\n",
      "copied ./data/validation/horses/58_horse5-565.png\n",
      "copied ./data/validation/horses/59_horse3-011.png\n",
      "copied ./data/validation/horses/60_horse4-548.png\n",
      "copied ./data/validation/horses/61_horse4-188.png\n",
      "copied ./data/validation/horses/62_horse6-403.png\n",
      "copied ./data/validation/horses/63_horse1-264.png\n",
      "copied ./data/validation/horses/64_horse3-541.png\n",
      "copied ./data/validation/horses/65_horse2-218.png\n",
      "copied ./data/validation/horses/66_horse3-217.png\n",
      "copied ./data/validation/horses/67_horse3-055.png\n",
      "copied ./data/validation/horses/68_horse2-254.png\n",
      "copied ./data/validation/horses/69_horse4-072.png\n",
      "copied ./data/validation/horses/70_horse1-276.png\n",
      "copied ./data/validation/horses/71_horse5-164.png\n",
      "copied ./data/validation/horses/72_horse5-083.png\n",
      "copied ./data/validation/horses/73_horse4-345.png\n",
      "copied ./data/validation/horses/74_horse5-065.png\n",
      "copied ./data/validation/horses/75_horse1-384.png\n",
      "copied ./data/validation/horses/76_horse6-064.png\n",
      "copied ./data/validation/horses/77_horse2-383.png\n",
      "copied ./data/validation/horses/78_horse1-241.png\n",
      "copied ./data/validation/horses/79_horse4-202.png\n",
      "copied ./data/validation/horses/80_horse6-198.png\n",
      "copied ./data/validation/horses/81_horse5-478.png\n",
      "copied ./data/validation/horses/82_horse3-498.png\n",
      "copied ./data/validation/horses/83_horse4-389.png\n",
      "copied ./data/validation/horses/84_horse5-519.png\n",
      "copied ./data/validation/horses/85_horse4-000.png\n",
      "copied ./data/validation/horses/86_horse4-556.png\n",
      "copied ./data/validation/horses/87_horse4-439.png\n",
      "copied ./data/validation/horses/88_horse6-275.png\n",
      "copied ./data/validation/horses/89_horse1-298.png\n",
      "copied ./data/validation/horses/90_horse2-011.png\n",
      "copied ./data/validation/horses/91_horse1-568.png\n",
      "copied ./data/validation/horses/92_horse3-584.png\n",
      "copied ./data/validation/horses/93_horse4-503.png\n",
      "copied ./data/validation/horses/94_horse2-136.png\n",
      "copied ./data/validation/horses/95_horse1-335.png\n",
      "copied ./data/validation/horses/96_horse4-530.png\n",
      "copied ./data/validation/horses/97_horse3-171.png\n",
      "copied ./data/validation/horses/98_horse5-076.png\n",
      "copied ./data/validation/horses/99_horse1-539.png\n",
      "copied ./data/validation/horses/100_horse6-345.png\n",
      "copied ./data/validation/horses/101_horse5-203.png\n",
      "copied ./data/validation/horses/102_horse6-004.png\n",
      "copied ./data/validation/horses/103_horse4-501.png\n",
      "copied ./data/validation/horses/104_horse2-224.png\n",
      "copied ./data/validation/horses/105_horse3-070.png\n",
      "copied ./data/validation/horses/106_horse5-032.png\n",
      "copied ./data/validation/horses/107_horse2-314.png\n",
      "copied ./data/validation/horses/108_horse1-122.png\n",
      "copied ./data/validation/horses/109_horse2-040.png\n",
      "copied ./data/validation/horses/110_horse2-596.png\n",
      "copied ./data/validation/horses/111_horse3-397.png\n",
      "copied ./data/validation/horses/112_horse5-458.png\n",
      "copied ./data/validation/horses/113_horse4-495.png\n",
      "copied ./data/validation/horses/114_horse5-100.png\n",
      "copied ./data/validation/horses/115_horse3-469.png\n",
      "copied ./data/validation/horses/116_horse2-112.png\n",
      "copied ./data/validation/horses/117_horse4-468.png\n",
      "copied ./data/validation/horses/118_horse6-161.png\n",
      "copied ./data/validation/horses/119_horse5-360.png\n",
      "copied ./data/validation/horses/120_horse3-326.png\n",
      "copied ./data/validation/horses/121_horse5-405.png\n",
      "copied ./data/validation/horses/122_horse2-183.png\n",
      "copied ./data/validation/horses/123_horse4-541.png\n",
      "copied ./data/validation/horses/124_horse5-342.png\n",
      "copied ./data/validation/horses/125_horse5-504.png\n",
      "copied ./data/validation/horses/126_horse3-416.png\n",
      "copied ./data/validation/horses/127_horse5-018.png\n",
      "copied ./data/validation/horses/128_horse1-436.png\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # repeat the same code for other folders as well.\n",
    "# source_folder = r\"./data/validation/horses/\"\n",
    "# destination_folder = r\"./data/validation/horses/\"\n",
    "# i=0\n",
    "# # fetch all files\n",
    "# for file_name in os.listdir(source_folder):\n",
    "#     # construct full file path\n",
    "#     i=i+1\n",
    "#     source = source_folder + file_name\n",
    "#     destination = destination_folder + str(i) + '_' + file_name\n",
    "#     # copy only files\n",
    "#     if os.path.isfile(source):\n",
    "#         shutil.copy(source, destination)\n",
    "#         print('copied', destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS account:706553727873\n",
      "AWS region:us-west-2\n",
      "CPU times: user 374 ms, sys: 34.8 ms, total: 409 ms\n",
      "Wall time: 961 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'horse-or-human'\n",
    "\n",
    "role = get_execution_role() \n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 train data path:  s3://sagemaker-us-west-2-706553727873/horse-or-human/data/train\n",
      "s3 validation data path:  s3://sagemaker-us-west-2-706553727873/horse-or-human/data/validation\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "# s3_input_data = f's3://{bucket}/{prefix}/data'\n",
    "s3_train_data = S3Uploader.upload('data/train',f's3://{bucket}/{prefix}/data/train')\n",
    "s3_val_data = S3Uploader.upload('data/validation',f's3://{bucket}/{prefix}/data/validation')\n",
    "print('s3 train data path: ', s3_train_data)\n",
    "print('s3 validation data path: ', s3_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://sagemaker-us-west-2-706553727873/horse-or-human/data/train'\n",
    "s3_val_data = 's3://sagemaker-us-west-2-706553727873/horse-or-human/data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {                        # Required\n",
    "#         \"microbatches\": 4,\n",
    "        \"partitions\": 1,                   # Required\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,                      # Required\n",
    "    \"processes_per_host\" : 8,              # Required\n",
    "    # \"custom_mpi_options\" : \"--mca btl_vader_single_copy_mechanism none\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define PyTorch Estimator\n",
    "metric_definitions=[\n",
    "                   {'Name': 'train:error', 'Regex': 'train_loss: ([0-9\\.]+)'},\n",
    "                   {'Name': 'validation:error', 'Regex': 'val_loss : ([0-9\\.]+)'}\n",
    "                ]\n",
    "from sagemaker.pytorch import PyTorch\n",
    "estimator = PyTorch(entry_point='train_smp.py',\n",
    "                    source_dir='src',\n",
    "                    role=role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.p3.16xlarge',\n",
    "                    framework_version=\"1.10.0\",\n",
    "                    py_version=\"py38\",\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'epochs':5,\n",
    "                        'batch_size':8, \n",
    "                         'lr':3e-5,\n",
    "                         'gamma': 0.7\n",
    "                    },\n",
    "                    distribution={\n",
    "                        \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "                        \"mpi\": mpi_options\n",
    "                    },\n",
    "                    debugger_hook_config=False,\n",
    "#                     disable_profiler=True,\n",
    "                    metric_definitions = metric_definitions,\n",
    "                    base_job_name=\"smp-training\",\n",
    "                    max_run=2400,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-11 23:13:06 Starting - Starting the training job...ProfilerReport-1649718786: InProgress\n",
      "...\n",
      "2022-04-11 23:14:03 Starting - Preparing the instances for training.........\n",
      "2022-04-11 23:15:34 Downloading - Downloading input data......\n",
      "2022-04-11 23:16:30 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:15,029 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:15,104 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:15,111 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:15,640 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting vit_pytorch\u001b[0m\n",
      "\u001b[34mDownloading vit_pytorch-0.33.2-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting linformer\u001b[0m\n",
      "\u001b[34mDownloading linformer-0.2.1-py3-none-any.whl (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting ipywidgets\u001b[0m\n",
      "\u001b[34mDownloading ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (2.69.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.1.35)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchnet in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.8/site-packages (from vit_pytorch->-r requirements.txt (line 1)) (1.10.0+cu113)\u001b[0m\n",
      "\u001b[34mCollecting einops>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading einops-0.4.1-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from vit_pytorch->-r requirements.txt (line 1)) (0.11.1+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->-r requirements.txt (line 3)) (5.0.5)\u001b[0m\n",
      "\u001b[34mCollecting ipykernel>=4.5.1\u001b[0m\n",
      "\u001b[34mDownloading ipykernel-6.13.0-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbformat>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading nbformat-5.3.0-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-widgets>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mCollecting widgetsnbextension~=3.6.0\u001b[0m\n",
      "\u001b[34mDownloading widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->-r requirements.txt (line 3)) (7.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (3.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (4.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (1.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (1.20.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: visdom in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 8)) (0.1.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.8/site-packages (from smdebug->-r requirements.txt (line 9)) (4.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.16.32->sagemaker->-r requirements.txt (line 5)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.16.32->sagemaker->-r requirements.txt (line 5)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.24.0,>=1.23.10 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.16.32->sagemaker->-r requirements.txt (line 5)) (1.23.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker->-r requirements.txt (line 5)) (3.6.0)\u001b[0m\n",
      "\u001b[34mCollecting nest-asyncio\u001b[0m\n",
      "\u001b[34mDownloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting debugpy>=1.0\u001b[0m\n",
      "\u001b[34mDownloading debugpy-1.6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting ipython>=4.0.0\u001b[0m\n",
      "\u001b[34mDownloading ipython-8.2.0-py3-none-any.whl (750 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-client>=6.1.12\u001b[0m\n",
      "\u001b[34mDownloading jupyter_client-7.2.2-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 3)) (5.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 3)) (6.1)\u001b[0m\n",
      "\u001b[34mCollecting matplotlib-inline>=0.1\u001b[0m\n",
      "\u001b[34mDownloading matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting traitlets>=4.3.1\u001b[0m\n",
      "\u001b[34mDownloading traitlets-5.1.1-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (4.4.2)\u001b[0m\n",
      "\u001b[34mCollecting stack-data\u001b[0m\n",
      "\u001b[34mDownloading stack_data-0.2.0-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (3.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (59.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (2.7.1)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-core\u001b[0m\n",
      "\u001b[34mDownloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mCollecting fastjsonschema\u001b[0m\n",
      "\u001b[34mDownloading fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->sagemaker->-r requirements.txt (line 5)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.10->vit_pytorch->-r requirements.txt (line 1)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mCollecting notebook>=4.4.1\u001b[0m\n",
      "\u001b[34mDownloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker->-r requirements.txt (line 5)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 5)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 5)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 5)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 5)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->vit_pytorch->-r requirements.txt (line 1)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 8)) (1.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 8)) (22.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 8)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchfile in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 8)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 8)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.24.0,>=1.23.10->boto3>=1.16.32->sagemaker->-r requirements.txt (line 5)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (3.0.3)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi\u001b[0m\n",
      "\u001b[34mDownloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-client\u001b[0m\n",
      "\u001b[34mDownloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[34mCollecting terminado>=0.8.3\u001b[0m\n",
      "\u001b[34mDownloading terminado-0.13.3-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting Send2Trash>=1.8.0\u001b[0m\n",
      "\u001b[34mDownloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbconvert>=5\u001b[0m\n",
      "\u001b[34mDownloading nbconvert-6.5.0-py3-none-any.whl (561 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.8/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 8)) (2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->torchnet->-r requirements.txt (line 8)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->torchnet->-r requirements.txt (line 8)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->torchnet->-r requirements.txt (line 8)) (2.0.4)\u001b[0m\n",
      "\u001b[34mCollecting executing\u001b[0m\n",
      "\u001b[34mDownloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting pure-eval\u001b[0m\n",
      "\u001b[34mDownloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting asttokens\u001b[0m\n",
      "\u001b[34mDownloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4\u001b[0m\n",
      "\u001b[34mDownloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34mCollecting bleach\u001b[0m\n",
      "\u001b[34mDownloading bleach-5.0.0-py3-none-any.whl (160 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting defusedxml\u001b[0m\n",
      "\u001b[34mDownloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting mistune<2,>=0.8.1\u001b[0m\n",
      "\u001b[34mDownloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting pandocfilters>=1.4.1\u001b[0m\n",
      "\u001b[34mDownloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tinycss2\u001b[0m\n",
      "\u001b[34mDownloading tinycss2-1.1.1-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbclient>=0.5.0\u001b[0m\n",
      "\u001b[34mDownloading nbclient-0.5.13-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-pygments\u001b[0m\n",
      "\u001b[34mDownloading jupyterlab_pygments-0.2.0-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi-bindings\u001b[0m\n",
      "\u001b[34mDownloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (1.14.6)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2\u001b[0m\n",
      "\u001b[34mDownloading soupsieve-2.3.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting webencodings\u001b[0m\n",
      "\u001b[34mDownloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (2.20)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: traitlets, pure-eval, nest-asyncio, jupyter-core, fastjsonschema, executing, entrypoints, asttokens, webencodings, stack-data, soupsieve, nbformat, matplotlib-inline, jupyter-client, tinycss2, pandocfilters, nbclient, mistune, jupyterlab-pygments, ipython, defusedxml, debugpy, bleach, beautifulsoup4, argon2-cffi-bindings, terminado, Send2Trash, prometheus-client, nbconvert, ipykernel, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, einops, vit-pytorch, linformer, ipywidgets\n",
      "  Attempting uninstall: traitlets\u001b[0m\n",
      "\u001b[34mFound existing installation: traitlets 5.0.5\u001b[0m\n",
      "\u001b[34mUninstalling traitlets-5.0.5:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled traitlets-5.0.5\u001b[0m\n",
      "\u001b[34mAttempting uninstall: ipython\u001b[0m\n",
      "\u001b[34mFound existing installation: ipython 7.18.1\u001b[0m\n",
      "\u001b[34mUninstalling ipython-7.18.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled ipython-7.18.1\u001b[0m\n",
      "\n",
      "2022-04-11 23:20:31 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully installed Send2Trash-1.8.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 asttokens-2.0.5 beautifulsoup4-4.11.1 bleach-5.0.0 debugpy-1.6.0 defusedxml-0.7.1 einops-0.4.1 entrypoints-0.4 executing-0.8.3 fastjsonschema-2.15.3 ipykernel-6.13.0 ipython-8.2.0 ipywidgets-7.7.0 jupyter-client-7.2.2 jupyter-core-4.9.2 jupyterlab-pygments-0.2.0 jupyterlab-widgets-1.1.0 linformer-0.2.1 matplotlib-inline-0.1.3 mistune-0.8.4 nbclient-0.5.13 nbconvert-6.5.0 nbformat-5.3.0 nest-asyncio-1.5.5 notebook-6.4.10 pandocfilters-1.5.0 prometheus-client-0.14.1 pure-eval-0.2.2 soupsieve-2.3.2 stack-data-0.2.0 terminado-0.13.3 tinycss2-1.1.1 traitlets-5.1.1 vit-pytorch-0.33.2 webencodings-0.5.1 widgetsnbextension-3.6.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:25,647 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:25,647 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:25,651 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:25,652 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:25,653 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-04-11 23:20:25,728 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 8,\n",
      "        \"epochs\": 5,\n",
      "        \"gamma\": 0.7,\n",
      "        \"lr\": 3e-05,\n",
      "        \"mp_parameters\": {\n",
      "            \"partitions\": 1,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"ddp\": true\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"image/png\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"ContentType\": \"image/png\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smp-training-2022-04-11-23-13-05-798\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-706553727873/smp-training-2022-04-11-23-13-05-798/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_smp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_smp.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":8,\"epochs\":5,\"gamma\":0.7,\"lr\":3e-05,\"mp_parameters\":{\"ddp\":true,\"optimize\":\"speed\",\"partitions\":1,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_smp.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_smp\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-706553727873/smp-training-2022-04-11-23-13-05-798/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":8,\"epochs\":5,\"gamma\":0.7,\"lr\":3e-05,\"mp_parameters\":{\"ddp\":true,\"optimize\":\"speed\",\"partitions\":1,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smp-training-2022-04-11-23-13-05-798\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-706553727873/smp-training-2022-04-11-23-13-05-798/source/sourcedir.tar.gz\",\"module_name\":\"train_smp\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_smp.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"8\",\"--epochs\",\"5\",\"--gamma\",\"0.7\",\"--lr\",\"3e-05\",\"--mp_parameters\",\"ddp=True,optimize=speed,partitions=1,pipeline=interleaved,placement_strategy=spread\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_LR=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"optimize\":\"speed\",\"partitions\":1,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_CHANNEL_VAL -x SM_HP_BATCH_SIZE -x SM_HP_EPOCHS -x SM_HP_GAMMA -x SM_HP_LR -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train_smp.py --batch_size 8 --epochs 5 --gamma 0.7 --lr 3e-05 --mp_parameters ddp=True,optimize=speed,partitions=1,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34mData for JOB [41156,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41156,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-11 23:20:34.760: I smdistributed/modelparallel/torch/state_mod.py:106] [6] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-11 23:20:34.769: I smdistributed/modelparallel/torch/state_mod.py:106] [3] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-11 23:20:34.769: I smdistributed/modelparallel/torch/state_mod.py:106] [2] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-11 23:20:34.769: I smdistributed/modelparallel/torch/state_mod.py:106] [1] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-11 23:20:34.769: I smdistributed/modelparallel/torch/state_mod.py:106] [4] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:34.770: I smdistributed/modelparallel/torch/state_mod.py:106] [0] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-11 23:20:34.770: I smdistributed/modelparallel/torch/state_mod.py:106] [5] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-11 23:20:34.770: I smdistributed/modelparallel/torch/state_mod.py:106] [7] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:DP Rank 7 Training dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:DP Rank 6 Training dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:DP Rank 0 Training dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- Gettting Validation Data Loader -------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:--------- Validation Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:---------------- Validation Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    Number of datapoints: 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    Root location: /opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               CenterCrop(size=(224, 224))\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Validation dataset and data loader length:  512 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>: [1,mpirank:3,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:DP Rank 1 Training dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>: [1,mpirank:3,algo-1]<stdout>:DP Rank 3 Training dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:DP Rank 2 Training dataset and data loader length:  2054 32[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:DP Rank 4 Training dataset and data loader length:  2054 [1,mpirank:4,algo-1]<stdout>:32\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:DP Rank 5 Training dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:number of params: 344804354\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:40.495: I smdistributed/modelparallel/torch/worker.py:280] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:43.097: I smdistributed/modelparallel/torch/model.py:231] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:43.098: I smdistributed/modelparallel/torch/model.py:238] main: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:43.105: I smdistributed/modelparallel/torch/model.py:180] Number of parameters on partition 0 are 188. 188 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:43.591: I smdistributed/modelparallel/torch/model.py:260] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:40 [2] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:41 [3] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:44 [6] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:45 [7] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:42 [4] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:43 [5] NCCL INFO Bootstrap : Using eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:40 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:42 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:41 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:45 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:44 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:43 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:40 [2] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:40 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:40 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:40 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:44 [6] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:43 [5] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:42 [4] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:45 [7] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:41 [3] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:41 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:44 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:43 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:42 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:45 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:41 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:41 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:44 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:45 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:43 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:42 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.101.195<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:42 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:44 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:45 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:43 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 01 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 01 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 09 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 09 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 02 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 02 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 10 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 02 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 02 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 10 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 10 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 10 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 03 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 03 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO Channel 11 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO Channel 11 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 03 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 03 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 03 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 11 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 03 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 11 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 11 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 11 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 05 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 13 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 05 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 05 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 05 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 13 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO Channel 13 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO Channel 13 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 05 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 13 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 05 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 06 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 13 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 14 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 06 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO Channel 14 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 06 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 14 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 06 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO Channel 14 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 07 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 07 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO Channel 15 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO Channel 15 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:42:912 [4] NCCL INFO comm 0x7fcf54002fb0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:907 [1] NCCL INFO comm 0x7f40b0002fb0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:41:908 [3] NCCL INFO comm 0x7f5aac002fb0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:40:906 [2] NCCL INFO comm 0x7f9428002fb0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:45:911 [7] NCCL INFO comm 0x7f8da0002fb0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:44:910 [6] NCCL INFO comm 0x7f7488002fb0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:905 [0] NCCL INFO comm 0x7f5ce0002fb0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:43:909 [5] NCCL INFO comm 0x7fd22c002fb0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:38:38 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-11 23:20:44.900: I smdistributed/modelparallel/torch/model.py:268] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:train_smp.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  loss = F.nll_loss(F.log_softmax(output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 0 train_loss: 0.49581634998321533)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 2 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 4 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 6 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 8 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 10 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 12 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 14 train_loss: 0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 16 train_loss: 25.256715774536133)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 18 train_loss: 20.65691375732422)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 20 train_loss: 14.03632926940918)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 22 train_loss: 6.749871730804443)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 24 train_loss: 0.4644991159439087)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 26 train_loss: 0.0014246234204620123)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 28 train_loss: 2.69558368017897e-05)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 1 Batch: 30 train_loss: 1.922247975016944e-06)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- Start Model Evaluation for the Epoch 1 -------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------- Test Function - Model Evaluation ----------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:----------- number of validation data batches ----------  64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- data parallel ---------- 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:train_smp.py:159: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  val_loss = F.nll_loss(F.log_softmax(val_output), label, reduction=\"mean\")\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Calculate Validation Loss ---------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Validation Distributed Loss calculation Completed -------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch : 1 - val_loss : 7.4990\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 0 train_loss: 14.958294868469238)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 2 train_loss: 14.081354141235352)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 4 train_loss: 12.13525676727295)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 6 train_loss: 9.792680740356445)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 8 train_loss: 7.364596843719482)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 10 train_loss: 4.9629621505737305)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 12 train_loss: 2.6747052669525146)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 14 train_loss: 0.8798935413360596)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 16 train_loss: 1.8233171701431274)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 18 train_loss: 2.8053224086761475)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 20 train_loss: 3.0899763107299805)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 22 train_loss: 2.8668668270111084)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 24 train_loss: 2.308098316192627)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 26 train_loss: 1.5831825733184814)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 28 train_loss: 0.8964553475379944)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 2 Batch: 30 train_loss: 0.4206683039665222)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- Start Model Evaluation for the Epoch 2 -------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------- Test Function - Model Evaluation ----------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:----------- number of validation data batches ----------  64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- data parallel ---------- 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Calculate Validation Loss ---------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Validation Distributed Loss calculation Completed -------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch : 2 - val_loss : 0.9880\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 0 train_loss: 1.7846976518630981)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 2 train_loss: 2.0872583389282227)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 4 train_loss: 2.122103214263916)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 6 train_loss: 1.9345874786376953)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 8 train_loss: 1.602048397064209)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 10 train_loss: 1.2007933855056763)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 12 train_loss: 0.8210399150848389)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 14 train_loss: 0.5128978490829468)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 16 train_loss: 1.3008478879928589)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 18 train_loss: 1.54913330078125)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 20 train_loss: 1.597192645072937)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 22 train_loss: 1.4738718271255493)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 24 train_loss: 1.23946213722229)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 26 train_loss: 0.949905276298523)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 28 train_loss: 0.670756459236145)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 3 Batch: 30 train_loss: 0.44341588020324707)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- Start Model Evaluation for the Epoch 3 -------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------- Test Function - Model Evaluation ----------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:----------- number of validation data batches ----------  64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- data parallel ---------- 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Calculate Validation Loss ---------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Validation Distributed Loss calculation Completed -------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch : 3 - val_loss : 0.8410\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 0 train_loss: 1.3930232524871826)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 2 train_loss: 1.5510153770446777)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 4 train_loss: 1.5730446577072144)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 6 train_loss: 1.4791761636734009)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 8 train_loss: 1.3013317584991455)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 10 train_loss: 1.0742928981781006)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 12 train_loss: 0.8432257771492004)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 14 train_loss: 0.6282976865768433)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 16 train_loss: 0.9931868314743042)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 18 train_loss: 1.1474521160125732)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 20 train_loss: 1.197810411453247)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 22 train_loss: 1.1502383947372437)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 24 train_loss: 1.0330862998962402)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 26 train_loss: 0.8715066909790039)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 28 train_loss: 0.6998702883720398)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 4 Batch: 30 train_loss: 0.5372487306594849)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- Start Model Evaluation for the Epoch 4 -------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------- Test Function - Model Evaluation ----------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:----------- number of validation data batches ----------  64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- data parallel ---------- 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Calculate Validation Loss ---------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Validation Distributed Loss calculation Completed -------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch : 4 - val_loss : 0.7541\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 0 train_loss: 1.1026345491409302)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 2 train_loss: 1.203818440437317)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 4 train_loss: 1.2296679019927979)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 6 train_loss: 1.182837724685669)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 8 train_loss: 1.085080862045288)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 10 train_loss: 0.9521222114562988)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 12 train_loss: 0.8106722235679626)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 14 train_loss: 0.6651241183280945)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 16 train_loss: 0.8675875663757324)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 18 train_loss: 0.9653372168540955)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 20 train_loss: 0.9988288283348083)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 22 train_loss: 0.9735960364341736)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 24 train_loss: 0.9035351872444153)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 26 train_loss: 0.8025829195976257)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 28 train_loss: 0.6905145049095154)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(Epoch: 5 Batch: 30 train_loss: 0.5762417316436768)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- Start Model Evaluation for the Epoch 5 -------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------- Test Function - Model Evaluation ----------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:----------- number of validation data batches ----------  64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-------------- data parallel ---------- 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Calculate Validation Loss ---------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:------------ Validation Distributed Loss calculation Completed -------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Epoch : 5 - val_loss : 0.7247\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m2022-04-11 23:25:54,529 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-04-11 23:26:14 Uploading - Uploading generated training model\n",
      "2022-04-11 23:26:33 Completed - Training job completed\n",
      "ProfilerReport-1649718786: IssuesFound\n",
      "Training seconds: 645\n",
      "Billable seconds: 645\n",
      "CPU times: user 1.77 s, sys: 147 ms, total: 1.92 s\n",
      "Wall time: 13min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train = TrainingInput(s3_train_data, content_type='image/png',input_mode='File')\n",
    "val = TrainingInput(s3_val_data, content_type='image/png',input_mode='File')\n",
    "estimator.fit({'train':train, 'val': val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-706553727873/pytorch-training-2022-04-04-23-39-49-889/output/model.tar.gz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
