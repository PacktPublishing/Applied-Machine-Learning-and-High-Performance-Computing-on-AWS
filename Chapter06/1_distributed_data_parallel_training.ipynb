{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data Parallel Training on Amazon SageMaker\n",
    "In this notebook we will use a Visual transformer to do image classification `horse or human` data from https://laurencemoroney.com/datasets.html. We will download both training and validation dataset provided on the site. \n",
    "\n",
    "Note: \n",
    "- Kernel: `PyTorch 1.8 Python 3.6 CPU Optimized)`\n",
    "- Instance Type: `ml.m5.xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  142M  100  142M    0     0  96.2M      0  0:00:01  0:00:01 --:--:-- 96.2M\n"
     ]
    }
   ],
   "source": [
    "## Download data\n",
    "!curl -o train.zip https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.9M  100 10.9M    0     0  20.3M      0 --:--:-- --:--:-- --:--:-- 20.3M\n"
     ]
    }
   ],
   "source": [
    "!curl -o validation.zip https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unzip file\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"train.zip\",\"r\") as train_zip_ref:\n",
    "    train_zip_ref.extractall(\"data/train\")\n",
    "    \n",
    "with zipfile.ZipFile(\"validation.zip\",\"r\") as val_zip_ref:\n",
    "    val_zip_ref.extractall(\"data/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to High Resolution\n",
    "We will start with converting our images to High Resolution using HuggingFace model [EdsrModel](#https://huggingface.co/eugenesiow/edsr-base) from `super-image` library. \n",
    "Please note that this step is optional, and reason for doing this is to mimick the real world image datasets for High Performance Computing, where image size might be in mega bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "     |████████████████████████████████| 325 kB 8.0 MB/s            \n",
      "\u001b[?25hCollecting super-image\n",
      "  Downloading super_image-0.1.6-py3-none-any.whl (85 kB)\n",
      "     |████████████████████████████████| 85 kB 109.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (2.26.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "     |████████████████████████████████| 211 kB 110.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets) (1.19.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets) (21.3)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 109.6 MB/s            \n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 104.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets) (4.8.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (2021.7.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
      "     |████████████████████████████████| 831.4 MB 159.8 MB/s            \n",
      "\u001b[?25hCollecting super-image\n",
      "  Downloading super_image-0.1.5-py3-none-any.whl (84 kB)\n",
      "     |████████████████████████████████| 84 kB 102.8 MB/s            \n",
      "\u001b[?25h  Downloading super_image-0.1.4-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 117.0 MB/s            \n",
      "\u001b[?25h  Downloading super_image-0.1.3-py3-none-any.whl (43 kB)\n",
      "     |████████████████████████████████| 43 kB 78.7 MB/s             \n",
      "\u001b[?25h  Downloading super_image-0.1.2-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 100.1 MB/s           \n",
      "\u001b[?25h  Downloading super_image-0.1.1-py3-none-any.whl (32 kB)\n",
      "  Downloading super_image-0.1.0-py3-none-any.whl (27 kB)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
      "     |████████████████████████████████| 312 kB 106.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
      "     |████████████████████████████████| 311 kB 141.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
      "     |████████████████████████████████| 312 kB 59.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
      "     |████████████████████████████████| 311 kB 109.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
      "     |████████████████████████████████| 311 kB 141.3 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 132.3 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
      "     |████████████████████████████████| 298 kB 112.2 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
      "     |████████████████████████████████| 298 kB 56.3 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 119.7 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 90.6 MB/s            \n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.19\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "     |████████████████████████████████| 56 kB 107.9 MB/s            \n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 137.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 124.5 MB/s            \n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 136.8 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 93.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
      "     |████████████████████████████████| 285 kB 137.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "     |████████████████████████████████| 270 kB 134.0 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
      "     |████████████████████████████████| 269 kB 128.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "     |████████████████████████████████| 264 kB 141.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /opt/conda/lib/python3.6/site-packages (from datasets) (4.61.2)\n",
      "Collecting torchvision==0.10.0\n",
      "  Downloading torchvision-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (22.1 MB)\n",
      "     |████████████████████████████████| 22.1 MB 34.6 MB/s            \n",
      "\u001b[?25hCollecting h5py==3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 63.5 MB/s            \n",
      "\u001b[?25hCollecting opencv-python==4.5.2.54\n",
      "  Downloading opencv_python-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (51.0 MB)\n",
      "     |████████████████████████████████| 51.0 MB 943 kB/s            \n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.13-py3-none-any.whl (38 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch==1.9.0->super-image) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.10.0->super-image) (8.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: torch, cached-property, xxhash, torchvision, opencv-python, huggingface-hub, h5py, super-image, datasets\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.1\n",
      "    Uninstalling torch-1.8.1:\n",
      "      Successfully uninstalled torch-1.8.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.1\n",
      "    Uninstalling torchvision-0.9.1:\n",
      "      Successfully uninstalled torchvision-0.9.1\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.4.60\n",
      "    Uninstalling opencv-python-4.5.4.60:\n",
      "      Successfully uninstalled opencv-python-4.5.4.60\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.8.0\n",
      "    Uninstalling h5py-2.8.0:\n",
      "      Successfully uninstalled h5py-2.8.0\n",
      "Successfully installed cached-property-1.5.2 datasets-1.11.0 h5py-3.1.0 huggingface-hub-0.0.13 opencv-python-4.5.2.54 super-image-0.1.6 torch-1.9.0 torchvision-0.10.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (2.72.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.82.2.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 9.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "     |████████████████████████████████| 49 kB 94.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.20.24)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.19.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (4.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (1.23.24)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.24->boto3>=1.20.21->sagemaker) (1.26.6)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.82.2-py2.py3-none-any.whl size=719305 sha256=2fc639a13c6d919237b9c7853d92a18c15f16bd2363e63691c7e9f8d2be2f5e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0u7vzt5x/wheels/a2/6f/5e/f1463fedfe4a5ef991b54e63d3fa7824970260aeb605872e2d\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: attrs, sagemaker\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.72.0\n",
      "    Uninstalling sagemaker-2.72.0:\n",
      "      Successfully uninstalled sagemaker-2.72.0\n",
      "Successfully installed attrs-20.3.0 sagemaker-2.82.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets super-image\n",
    "!python3 -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using `horse-or-human` dataset from here, which has image size of about `178KB`, we will convert these images to Higher resolution and `resulting size would be close to 2MB`. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@InProceedings{Lim_2017_CVPR_Workshops,\n",
    "  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n",
    "  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n",
    "  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n",
    "  month = {July},\n",
    "  year = {2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/eugenesiow/edsr-base/resolve/main/pytorch_model_4x.pt\n"
     ]
    }
   ],
   "source": [
    "from super_image import EdsrModel, ImageLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "folder_dir = \"data/validation/\"\n",
    "for folder in os.listdir(folder_dir):\n",
    "    folder_path = f'{folder_dir}{folder}'\n",
    "    for image_file in os.listdir(folder_path):\n",
    "        path = f'{folder_path}/{image_file}'\n",
    "        image = Image.open(path)\n",
    "        inputs = ImageLoader.load_image(image)\n",
    "        preds = model(inputs)\n",
    "        ImageLoader.save_image(preds, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size is : 0.481116 MB\n"
     ]
    }
   ],
   "source": [
    "# quick check on image size for the last image converted by the model.\n",
    "import os\n",
    "\n",
    "file_size = os.path.getsize(path)\n",
    "print(\"File Size is :\", file_size/1000000, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Duplicate files to increase number of images for testing purpose in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied ./data/validation/horses/1_horse5-235.png\n",
      "copied ./data/validation/horses/2_horse5-550.png\n",
      "copied ./data/validation/horses/3_horse1-554.png\n",
      "copied ./data/validation/horses/4_horse6-153.png\n",
      "copied ./data/validation/horses/5_horse5-514.png\n",
      "copied ./data/validation/horses/6_horse3-255.png\n",
      "copied ./data/validation/horses/7_horse1-455.png\n",
      "copied ./data/validation/horses/8_horse5-181.png\n",
      "copied ./data/validation/horses/9_horse2-544.png\n",
      "copied ./data/validation/horses/10_horse5-002.png\n",
      "copied ./data/validation/horses/11_horse1-204.png\n",
      "copied ./data/validation/horses/12_horse1-105.png\n",
      "copied ./data/validation/horses/13_horse4-102.png\n",
      "copied ./data/validation/horses/14_horse1-411.png\n",
      "copied ./data/validation/horses/15_horse6-089.png\n",
      "copied ./data/validation/horses/16_horse1-510.png\n",
      "copied ./data/validation/horses/17_horse5-275.png\n",
      "copied ./data/validation/horses/18_horse4-159.png\n",
      "copied ./data/validation/horses/19_horse3-484.png\n",
      "copied ./data/validation/horses/20_horse2-368.png\n",
      "copied ./data/validation/horses/21_horse6-544.png\n",
      "copied ./data/validation/horses/22_horse2-269.png\n",
      "copied ./data/validation/horses/23_horse5-402.png\n",
      "copied ./data/validation/horses/24_horse4-043.png\n",
      "copied ./data/validation/horses/25_horse2-441.png\n",
      "copied ./data/validation/horses/26_horse4-599.png\n",
      "copied ./data/validation/horses/27_horse3-521.png\n",
      "copied ./data/validation/horses/28_horse5-123.png\n",
      "copied ./data/validation/horses/29_horse4-403.png\n",
      "copied ./data/validation/horses/30_horse3-440.png\n",
      "copied ./data/validation/horses/31_horse1-127.png\n",
      "copied ./data/validation/horses/32_horse3-026.png\n",
      "copied ./data/validation/horses/33_horse5-303.png\n",
      "copied ./data/validation/horses/34_horse5-060.png\n",
      "copied ./data/validation/horses/35_horse5-589.png\n",
      "copied ./data/validation/horses/36_horse4-232.png\n",
      "copied ./data/validation/horses/37_horse1-000.png\n",
      "copied ./data/validation/horses/38_horse5-192.png\n",
      "copied ./data/validation/horses/39_horse2-294.png\n",
      "copied ./data/validation/horses/40_horse6-218.png\n",
      "copied ./data/validation/horses/41_horse1-484.png\n",
      "copied ./data/validation/horses/42_horse2-069.png\n",
      "copied ./data/validation/horses/43_horse2-582.png\n",
      "copied ./data/validation/horses/44_horse5-400.png\n",
      "copied ./data/validation/horses/45_horse4-302.png\n",
      "copied ./data/validation/horses/46_horse5-103.png\n",
      "copied ./data/validation/horses/47_horse1-224.png\n",
      "copied ./data/validation/horses/48_horse3-141.png\n",
      "copied ./data/validation/horses/49_horse5-488.png\n",
      "copied ./data/validation/horses/50_horse1-170.png\n",
      "copied ./data/validation/horses/51_horse2-412.png\n",
      "copied ./data/validation/horses/52_horse4-014.png\n",
      "copied ./data/validation/horses/53_horse4-588.png\n",
      "copied ./data/validation/horses/54_horse2-201.png\n",
      "copied ./data/validation/horses/55_horse3-198.png\n",
      "copied ./data/validation/horses/56_horse3-099.png\n",
      "copied ./data/validation/horses/57_horse5-259.png\n",
      "copied ./data/validation/horses/58_horse5-565.png\n",
      "copied ./data/validation/horses/59_horse3-011.png\n",
      "copied ./data/validation/horses/60_horse4-548.png\n",
      "copied ./data/validation/horses/61_horse4-188.png\n",
      "copied ./data/validation/horses/62_horse6-403.png\n",
      "copied ./data/validation/horses/63_horse1-264.png\n",
      "copied ./data/validation/horses/64_horse3-541.png\n",
      "copied ./data/validation/horses/65_horse2-218.png\n",
      "copied ./data/validation/horses/66_horse3-217.png\n",
      "copied ./data/validation/horses/67_horse3-055.png\n",
      "copied ./data/validation/horses/68_horse2-254.png\n",
      "copied ./data/validation/horses/69_horse4-072.png\n",
      "copied ./data/validation/horses/70_horse1-276.png\n",
      "copied ./data/validation/horses/71_horse5-164.png\n",
      "copied ./data/validation/horses/72_horse5-083.png\n",
      "copied ./data/validation/horses/73_horse4-345.png\n",
      "copied ./data/validation/horses/74_horse5-065.png\n",
      "copied ./data/validation/horses/75_horse1-384.png\n",
      "copied ./data/validation/horses/76_horse6-064.png\n",
      "copied ./data/validation/horses/77_horse2-383.png\n",
      "copied ./data/validation/horses/78_horse1-241.png\n",
      "copied ./data/validation/horses/79_horse4-202.png\n",
      "copied ./data/validation/horses/80_horse6-198.png\n",
      "copied ./data/validation/horses/81_horse5-478.png\n",
      "copied ./data/validation/horses/82_horse3-498.png\n",
      "copied ./data/validation/horses/83_horse4-389.png\n",
      "copied ./data/validation/horses/84_horse5-519.png\n",
      "copied ./data/validation/horses/85_horse4-000.png\n",
      "copied ./data/validation/horses/86_horse4-556.png\n",
      "copied ./data/validation/horses/87_horse4-439.png\n",
      "copied ./data/validation/horses/88_horse6-275.png\n",
      "copied ./data/validation/horses/89_horse1-298.png\n",
      "copied ./data/validation/horses/90_horse2-011.png\n",
      "copied ./data/validation/horses/91_horse1-568.png\n",
      "copied ./data/validation/horses/92_horse3-584.png\n",
      "copied ./data/validation/horses/93_horse4-503.png\n",
      "copied ./data/validation/horses/94_horse2-136.png\n",
      "copied ./data/validation/horses/95_horse1-335.png\n",
      "copied ./data/validation/horses/96_horse4-530.png\n",
      "copied ./data/validation/horses/97_horse3-171.png\n",
      "copied ./data/validation/horses/98_horse5-076.png\n",
      "copied ./data/validation/horses/99_horse1-539.png\n",
      "copied ./data/validation/horses/100_horse6-345.png\n",
      "copied ./data/validation/horses/101_horse5-203.png\n",
      "copied ./data/validation/horses/102_horse6-004.png\n",
      "copied ./data/validation/horses/103_horse4-501.png\n",
      "copied ./data/validation/horses/104_horse2-224.png\n",
      "copied ./data/validation/horses/105_horse3-070.png\n",
      "copied ./data/validation/horses/106_horse5-032.png\n",
      "copied ./data/validation/horses/107_horse2-314.png\n",
      "copied ./data/validation/horses/108_horse1-122.png\n",
      "copied ./data/validation/horses/109_horse2-040.png\n",
      "copied ./data/validation/horses/110_horse2-596.png\n",
      "copied ./data/validation/horses/111_horse3-397.png\n",
      "copied ./data/validation/horses/112_horse5-458.png\n",
      "copied ./data/validation/horses/113_horse4-495.png\n",
      "copied ./data/validation/horses/114_horse5-100.png\n",
      "copied ./data/validation/horses/115_horse3-469.png\n",
      "copied ./data/validation/horses/116_horse2-112.png\n",
      "copied ./data/validation/horses/117_horse4-468.png\n",
      "copied ./data/validation/horses/118_horse6-161.png\n",
      "copied ./data/validation/horses/119_horse5-360.png\n",
      "copied ./data/validation/horses/120_horse3-326.png\n",
      "copied ./data/validation/horses/121_horse5-405.png\n",
      "copied ./data/validation/horses/122_horse2-183.png\n",
      "copied ./data/validation/horses/123_horse4-541.png\n",
      "copied ./data/validation/horses/124_horse5-342.png\n",
      "copied ./data/validation/horses/125_horse5-504.png\n",
      "copied ./data/validation/horses/126_horse3-416.png\n",
      "copied ./data/validation/horses/127_horse5-018.png\n",
      "copied ./data/validation/horses/128_horse1-436.png\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # repeat the same code for other folders as well.\n",
    "# source_folder = r\"./data/validation/horses/\"\n",
    "# destination_folder = r\"./data/validation/horses/\"\n",
    "# i=0\n",
    "# # fetch all files\n",
    "# for file_name in os.listdir(source_folder):\n",
    "#     # construct full file path\n",
    "#     i=i+1\n",
    "#     source = source_folder + file_name\n",
    "#     destination = destination_folder + str(i) + '_' + file_name\n",
    "#     # copy only files\n",
    "#     if os.path.isfile(source):\n",
    "#         shutil.copy(source, destination)\n",
    "#         print('copied', destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS account:706553727873\n",
      "AWS region:us-west-2\n",
      "CPU times: user 336 ms, sys: 17 ms, total: 353 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'horse-or-human'\n",
    "\n",
    "role = get_execution_role() \n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 train data path:  s3://sagemaker-us-west-2-706553727873/horse-or-human/data/train\n",
      "s3 validation data path:  s3://sagemaker-us-west-2-706553727873/horse-or-human/data/validation\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "# s3_input_data = f's3://{bucket}/{prefix}/data'\n",
    "s3_train_data = S3Uploader.upload('data/train',f's3://{bucket}/{prefix}/data/train')\n",
    "s3_val_data = S3Uploader.upload('data/validation',f's3://{bucket}/{prefix}/data/validation')\n",
    "print('s3 train data path: ', s3_train_data)\n",
    "print('s3 validation data path: ', s3_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define PyTorch Estimator\n",
    "metric_definitions=[\n",
    "                   {'Name': 'train:error', 'Regex': 'loss : ([0-9\\.]+)'},\n",
    "                   {'Name': 'validation:error', 'Regex': 'val_loss : ([0-9\\.]+)'}\n",
    "                ]\n",
    "from sagemaker.pytorch import PyTorch\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='src',\n",
    "                    role=role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.p3.16xlarge',\n",
    "                    framework_version='1.8.0',\n",
    "                    py_version='py3',\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={'epochs':10,\n",
    "                                     'batch_size':32, \n",
    "                                     'lr':3e-5,\n",
    "                                     'gamma': 0.7},\n",
    "                    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "                    debugger_hook_config=False,\n",
    "                    metric_definitions = metric_definitions,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:39:50 Starting - Starting the training job...\n",
      "2022-04-04 23:40:14 Starting - Launching requested ML instancesProfilerReport-1649115589: InProgress\n",
      ".........\n",
      "2022-04-04 23:41:35 Starting - Preparing the instances for training......\n",
      "2022-04-04 23:42:51 Downloading - Downloading input data......\n",
      "2022-04-04 23:43:35 Training - Downloading the training image....................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:05,081 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:05,158 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:05,164 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:05,165 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:05,490 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting vit_pytorch\n",
      "  Downloading vit_pytorch-0.32.2-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34mCollecting linformer\n",
      "  Downloading linformer-0.2.1-py3-none-any.whl (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting ipywidgets\n",
      "  Downloading ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mCollecting widgetsnbextension~=3.6.0\n",
      "  Downloading widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 3)) (7.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 3)) (4.3.3)\u001b[0m\n",
      "\u001b[34mCollecting ipykernel>=4.5.1\n",
      "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbformat>=4.2.0\n",
      "  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 3)) (6.1)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-client\n",
      "  Downloading jupyter_client-7.1.2-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pexpect in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (3.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (2.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-core\n",
      "  Downloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34mCollecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 3)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 3)) (20.3.0)\u001b[0m\n",
      "\u001b[34mCollecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 3)) (3.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 3)) (0.2.5)\u001b[0m\n",
      "\u001b[34mCollecting notebook>=4.4.1\n",
      "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (22.0.3)\u001b[0m\n",
      "\u001b[34mCollecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting nest-asyncio>=1.5\u001b[0m\n",
      "\u001b[34m  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting terminado>=0.8.3\n",
      "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbconvert>=5\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (2.11.3)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-client\n",
      "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 3)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting bleach\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting testpath\n",
      "  Downloading testpath-0.6.0-py3-none-any.whl (83 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.9-py3-none-any.whl (69 kB)\u001b[0m\n",
      "\u001b[34mCollecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (1.1.1)\u001b[0m\n",
      "\u001b[34mCollecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.6/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from linformer->-r requirements.txt (line 2)) (1.8.0)\u001b[0m\n",
      "\u001b[34mCollecting einops>=0.4.1\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from vit_pytorch->-r requirements.txt (line 1)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch->linformer->-r requirements.txt (line 2)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->linformer->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->linformer->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (1.14.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (2.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (20.9)\u001b[0m\n",
      "\u001b[34mCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->vit_pytorch->-r requirements.txt (line 1)) (8.1.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyrsistent, nest-asyncio, jupyter-core, jsonschema, entrypoints, webencodings, nbformat, jupyter-client, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, defusedxml, bleach, argon2-cffi-bindings, terminado, Send2Trash, prometheus-client, nbconvert, ipykernel, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, einops, vit-pytorch, linformer, ipywidgets\u001b[0m\n",
      "\u001b[34mSuccessfully installed Send2Trash-1.8.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-generator-1.10 bleach-4.1.0 defusedxml-0.7.1 einops-0.4.1 entrypoints-0.4 ipykernel-5.5.6 ipywidgets-7.7.0 jsonschema-3.2.0 jupyter-client-7.1.2 jupyter-core-4.9.2 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.1.0 linformer-0.2.1 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.5.5 notebook-6.4.10 pandocfilters-1.5.0 prometheus-client-0.13.1 pyrsistent-0.18.0 terminado-0.12.1 testpath-0.6.0 vit-pytorch-0.32.2 webencodings-0.5.1 widgetsnbextension-3.6.0\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:14,121 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:14,121 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:14,124 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:14,124 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:14,124 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2022-04-04 23:47:14,204 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"lr\": 3e-05,\n",
      "        \"epochs\": 10,\n",
      "        \"gamma\": 0.7\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"ContentType\": \"image/png\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"image/png\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-04-23-39-49-889\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-706553727873/pytorch-training-2022-04-04-23-39-49-889/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":32,\"epochs\":10,\"gamma\":0.7,\"lr\":3e-05}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-706553727873/pytorch-training-2022-04-04-23-39-49-889/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":32,\"epochs\":10,\"gamma\":0.7,\"lr\":3e-05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"image/png\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-04-23-39-49-889\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-706553727873/pytorch-training-2022-04-04-23-39-49-889/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"32\",\"--epochs\",\"10\",\"--gamma\",\"0.7\",\"--lr\",\"3e-05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_LR=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=0.7\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.6 -m mpi4py train.py --batch_size 32 --epochs 10 --gamma 0.7 --lr 3e-05\u001b[0m\n",
      "\n",
      "2022-04-04 23:47:16 Training - Training image download completed. Training in progress.\u001b[34m[1,0]<stdout>:NCCL version 2.8.4+cuda11.1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.1.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Hello from rank 0 of local_rank 0 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:--------- Validation Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Hello from rank 7 of local_rank 7 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:---------------- Validation Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               Resize(size=256, interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               CenterCrop(size=(224, 224))\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Validation dataset and data loader length:  2054 32\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Hello from rank 5 of local_rank 5 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Hello from rank 1 of local_rank 1 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Hello from rank 4 of local_rank 4 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Hello from rank 6 of local_rank 6 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Hello from rank 2 of local_rank 2 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:training directory path:  /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Hello from rank 3 of local_rank 3 in world size of 8\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:--------- Training Initializing Dataloader ----------------\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:---------------- Train Dataset Info:-----------------\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>: Dataset CustomImageLoader\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    Number of datapoints: 2054\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    Root location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    StandardTransform\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Transform: Compose(\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               Resize(size=(224, 224), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               RandomHorizontalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               RandomGrayscale(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               RandomVerticalFlip(p=0.2)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               RandomInvert(p=0.1)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:               ToTensor()\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:           )\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train dataset and data loader length:  2054 8\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-04-04 23:47:26.040 algo-1:46 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2022-04-04 23:47:26.041 algo-1:52 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-04-04 23:47:26.042 algo-1:44 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-04-04 23:47:26.043 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-04-04 23:47:26.045 algo-1:53 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-04-04 23:47:26.045 algo-1:50 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2022-04-04 23:47:26.048 algo-1:48 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2022-04-04 23:47:26.065 algo-1:54 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-04-04 23:47:26.133 algo-1:42 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-04-04 23:47:26.133 algo-1:50 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2022-04-04 23:47:26.133 algo-1:52 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-04-04 23:47:26.133 algo-1:53 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-04-04 23:47:26.133 algo-1:44 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2022-04-04 23:47:26.133 algo-1:54 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-04-04 23:47:26.134 algo-1:46 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2022-04-04 23:47:26.136 algo-1:48 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [0/2054 (0%)]#011loss: 0.710812\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [512/2054 (25%)]#011loss: 0.703692\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [1024/2054 (50%)]#011loss: 0.744314\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [1536/2054 (75%)]#011loss: 0.694360\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 1 - val_loss : 0.6984 - val_acc: 0.4873\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [0/2054 (0%)]#011loss: 0.699471\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [512/2054 (25%)]#011loss: 0.692155\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 0 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 0 from rank 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 0 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 0 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [1024/2054 (50%)]#011loss: 0.694743\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 0 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 1 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 1 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 1 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 1 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 1 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 2 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 2 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 2 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 2 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [1536/2054 (75%)]#011loss: 0.686318\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 2 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 3 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 3 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 3 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 0 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 3 from rank 6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 3 from rank 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 4 from rank 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 4 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 4 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 0 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 1 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 4 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 5 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 4 from rank 4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 5 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 5 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 1 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 2 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 5 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 6 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 5 from rank 4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 6 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 6 from rank 3\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 2 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 3 from rank 7\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 6 from rank 6\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Batch 7 from rank 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 6 from rank 4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Batch 7 from rank 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Batch 7 from rank 3\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 4 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 3 from rank 5\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Batch 7 from rank 6\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Batch 7 from rank 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 5 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 4 from rank 5\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 5 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 6 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 6 from rank 5\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Batch 7 from rank 7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Batch 7 from rank 5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 2 - val_loss : 0.6892 - val_acc: 0.5142\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [0/2054 (0%)]#011loss: 0.680186\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [512/2054 (25%)]#011loss: 0.687618\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [1024/2054 (50%)]#011loss: 0.714914\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [1536/2054 (75%)]#011loss: 0.684782\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 3 - val_loss : 0.6885 - val_acc: 0.5122\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [0/2054 (0%)]#011loss: 0.681157\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [512/2054 (25%)]#011loss: 0.683449\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [1024/2054 (50%)]#011loss: 0.702173\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [1536/2054 (75%)]#011loss: 0.686748\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 4 - val_loss : 0.6862 - val_acc: 0.5542\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [0/2054 (0%)]#011loss: 0.681210\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [512/2054 (25%)]#011loss: 0.684874\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [1024/2054 (50%)]#011loss: 0.698829\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [1536/2054 (75%)]#011loss: 0.683361\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 5 - val_loss : 0.6858 - val_acc: 0.5591\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [0/2054 (0%)]#011loss: 0.675131\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [512/2054 (25%)]#011loss: 0.683183\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [1024/2054 (50%)]#011loss: 0.700456\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [1536/2054 (75%)]#011loss: 0.679045\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 6 - val_loss : 0.6853 - val_acc: 0.5415\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [0/2054 (0%)]#011loss: 0.678712\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [512/2054 (25%)]#011loss: 0.679561\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [1024/2054 (50%)]#011loss: 0.702025\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [1536/2054 (75%)]#011loss: 0.679813\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 7 - val_loss : 0.6850 - val_acc: 0.5352\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [0/2054 (0%)]#011loss: 0.674997\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [512/2054 (25%)]#011loss: 0.674580\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [1024/2054 (50%)]#011loss: 0.704933\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [1536/2054 (75%)]#011loss: 0.683500\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 8 - val_loss : 0.6849 - val_acc: 0.5347\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [0/2054 (0%)]#011loss: 0.673032\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [512/2054 (25%)]#011loss: 0.677808\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [1024/2054 (50%)]#011loss: 0.697607\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [1536/2054 (75%)]#011loss: 0.680675\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 9 - val_loss : 0.6848 - val_acc: 0.5342\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [0/2054 (0%)]#011loss: 0.682942\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 0 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 1 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [512/2054 (25%)]#011loss: 0.671785\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 2 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 3 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [1024/2054 (50%)]#011loss: 0.713084\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 4 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 5 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch idx:  6\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [1536/2054 (75%)]#011loss: 0.683738\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 6 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Batch 7 from rank 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch : 10 - val_loss : 0.6846 - val_acc: 0.5381\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saving model: /opt/ml/model/model.pth \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m2022-04-05 00:06:47,988 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-04-05 00:07:42 Uploading - Uploading generated training model\n",
      "2022-04-05 00:07:42 Completed - Training job completed\n",
      "ProfilerReport-1649115589: IssuesFound\n",
      "Training seconds: 1481\n",
      "Billable seconds: 1481\n",
      "CPU times: user 3.51 s, sys: 267 ms, total: 3.78 s\n",
      "Wall time: 28min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train = TrainingInput(s3_train_data, content_type='image/png',input_mode='File')\n",
    "val = TrainingInput(s3_val_data, content_type='image/png',input_mode='File')\n",
    "estimator.fit({'train':train, 'val': val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
